<!DOCTYPE html>
<html lang="en">

<head>
<meta charset="utf-8" />
<link rel="stylesheet" href="https://dokie.li/media/css/lncs.css" />
<title>Takeaways for Week 5</title>
</head>

<body>

<h1>Takeaways for Week 5</h1>

<h2>Topics</h2>

<h3>Knowledge Representation on the Web</h3>

<p class="counter">
The Web has high universal variety and coverage, it is structured by links, open for everyone and decentralized as there is no authority that controls it. These properties of the Web raises some challenges such as the low-quality data that the web includes, and users can define a concept several times as there are different types of users and creators who sometimes have no technical background. The three core languages of the Semantic WEb are RDF, SPARQL and OWL, where RDF and OWL can be expressed using different forms such as RDF/XML, OWL/XML and Turtle. In order to organize the knowledge in the Web environment, semantics and pragmatic modelling principles which fit the openness and decentralization of the Web are needed. </p>

<h3>Knowledge Modeling</h3>

<p class="counter">
Categorization, which means in this context the process of creating classification, is a type of knowledge modelling (acquisition). There are three types of categorization, the first type is individual categorization where this type occurs when someone makes a classification for their own use. The second type is cultural categorization which occurs when categories are shared by culture and associated with a language.  The third type is institutional categorization which serves institutional aims and requires time and effort. Knowledge can not be extracted , it can rather be modeled. However, knowledge modelling is a difficult process, it requires creativity as you understand and realize a specific concept (knowledge) but you can not easily model it. </p>

<h2>Literature</h2>

<h3>Bizer et al. 2009</h3>

<p class="counter">
Bizer et al. (2009) describe in their article the DBpedia knowledge framework whose data sources are extracted from Wikipedia, they also give an overview of the resulting DBpedia after interlinking it with other data sources on the web. DBpedia extracts the structured information included in Wikipedia, and it makes this structured information accessible on the web. Moreover, DBpedia is a representation of a real community agreement. It is a multilingual knowledge base(30 languages) covering many different domains as it describes more than 2.6 million entities (in 2009). These entities are described in DBpedia by providing a set of general properties and a set of infobox-specific properties. Furthermore, DBpedia is one of the central interlinking hubs of the Web of Data, this is resulted from the increasing number of the RDF links which are provided by data publishers to the DBpedia entities, and also the RDF linksthat DBpedia already has, where in 2009 it had 4.9 million outgoing RDF links.</p>

<h3>Vrandecic and Krotzsch 2014</h3>

<p class="counter">
The use of data included in Wikipedia is rare and restricted to certain pieces of information, this is because wikipedia data comes from 30 million Wikipedia articles  in 287 languages which makes the data extraction very difficult  (Vrandecic and Krotzsch 2014). Vrandecic and Krotzsch (2014) describe in their article the evolution of Wikidata, where Wikidata was launched by Wikimedia in 2012. The aim of Wikidata is to manage the data of Wikipedia on a global scale in order to overcome problems related to inconsistency, up-to-date and accurate knowledge. Wikidata is an easily accessible knowledge base where all users can edit and extend the existing information. Information included in Wikidata is controlled by the contributors community. Moreover, Wikidata provides data in many different languages, it also allows conflicting data to coexist. The data model used in Wikidata is a simple model where data is described through property-value pairs.</p>

<h3>Beek et al. 2016</h3>

<p class="counter">
Beek et al. (2016) describe the LOD Laundromat which is a centralized solution for the problems that Semantic Web has. Such problems is that Semantic Web is not machine-processable, and Web agents and applications can not traverse it. Another problem is that information from different sources is difficult to access. These problems can be solved by the LOD laundromat system which can achieve tasks related to data cleaning. LOD laundromat can clean the data and allow the download of this data, which makes LOD laundromat a republishing platform that provides a uniform access to large subcollections of the LOD cloud. Moreover, SPARQL is not an efficient approach to be used when querying from the Semantic Web as it returns incomplete results. LOD laundromat solves that problem by changing the processing from server-side to client-side with the aid of different data storage forms.</p>

<h3>Schreiber et al. 2008</h3>

<p class="counter">
The article of Schreiber et al. (2008) describes their project MultiMediaN E-cilture The aim of this project is to show that the Semantic web and presentation technologies can be used as an efficient tool for indexing and search support within the cultural-heritage domain. The cultural-heritage domain is chosen as it is a rich domain of vocabularies, the information is accessible and  cultural semantic annotations are available. The approach of the MultiMediaN E-culture project depends on three parts, the first one is to offer facilities to harvest, enrich and align collection metadata and vocabularies. The first part is achieved through making vocabulary interoperable by using RDF/OWL and SKOS, and aligning metadata schema by mapping it through the use of dump-down principles, and enriching metadata by finding matching concepts from thesauri. The second part of the approach is to provide a semantic search interface which includes the resulting graph by using SPARQL API. The third part is to allow the users to add metadata or content, but this part is not achieved yet (at 2008) and is discussed in the article as a further work. </p>

<h2>References</h2>

<ul>
<li>Bizer et al. 2009. <a href="http://dx.doi.org/10.1016/j.websem.2009.07.002">DBpedia - A crystallization point for the Web of Data.</a> Web Semantics, 7(3).</li>
<li>Vrandecic and Krotzsch. 2014. <a href="http://dx.doi.org/10.1145/2629489">Wikidata: a free collaborative knowledgebase.</a> Communications of the ACM 57(10).</li>
<li>Beek et al. 2016. <a href="http://dx.doi.org/10.1109/mic.2016.43">LOD Laundromat: Why the Semantic Web Needs Centralization (Even If We Donâ€™t Like It).</a> IEEE Internet Computing, 20(2).</li>
<li>Schreiber et al. 2008. <a href="http://dx.doi.org/10.1016/j.websem.2008.08.001">Semantic annotation and search of cultural-heritage collections: The MultimediaN E-Culture demonstrator.</a> Web Semantics, 6(4).</li>
</ul>

<script src="https://raw.githack.com/ucds-vu/ko2021-templates/main/scripts.js"></script>

</body>
</html>

